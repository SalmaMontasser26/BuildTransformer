{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95f2aa35",
   "metadata": {},
   "source": [
    "## Building a Transformer for a Multi-classification NLP Problem\n",
    "## Project Phases:\n",
    "### 1. Analysis & Cleaning\n",
    "### 2. Preparing Input & Target\n",
    "### 3. Building Model\n",
    "### 4. Training\n",
    "### 5. Testing\n",
    "### 6. Optemizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7e689d",
   "metadata": {},
   "source": [
    "###### Import libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c298f2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d61784",
   "metadata": {},
   "source": [
    "###### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93938f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "567b65e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc128c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   id             159571 non-null  object\n",
      " 1   comment_text   159571 non-null  object\n",
      " 2   toxic          159571 non-null  int64 \n",
      " 3   severe_toxic   159571 non-null  int64 \n",
      " 4   obscene        159571 non-null  int64 \n",
      " 5   threat         159571 non-null  int64 \n",
      " 6   insult         159571 non-null  int64 \n",
      " 7   identity_hate  159571 non-null  int64 \n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 9.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f55a4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>159571</td>\n",
       "      <td>159571</td>\n",
       "      <td>159571</td>\n",
       "      <td>159571</td>\n",
       "      <td>159571</td>\n",
       "      <td>159571</td>\n",
       "      <td>159571</td>\n",
       "      <td>159571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  comment_text   toxic  severe_toxic  obscene  threat  insult  \\\n",
       "False  159571        159571  159571        159571   159571  159571  159571   \n",
       "\n",
       "       identity_hate  \n",
       "False         159571  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shows if there are nulls in any column\n",
    "df.isna().apply(pd.value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7a41db",
   "metadata": {},
   "source": [
    "### What's concluded from above is as follow:\n",
    "- there are 8 columns and 159571 rows\n",
    "- id column has no use for the problem\n",
    "- comment_text column is of type string and has the comments to be cleaned, analysed and finally taken as an input for the encoder block of the transformer\n",
    "- the rest of the columns assign the comment's category with a 0 or 1 (might combine them all in one column to use them as the target for the transformer)\n",
    "- there are no nulls\n",
    "- the comments are filled with punctiuation and special characters (need to be removed)\n",
    "- of course not all comments are the same length which will require padding them\n",
    "\n",
    "### Next step:\n",
    "\n",
    "- \n",
    "#### Cleaning:\n",
    "    - start with primary cleaning (lower case data, remove links, emails, punctiuation, special characters, numbers, stop words) to allow faster computation and easier and further analysis (next step)\n",
    "- \n",
    "#### Analysis:\n",
    "    - determine frequency of words\n",
    "    - identify most and least common words\n",
    "    - spot any irregular phrases or patterns that might create noise (need to be removed)\n",
    "    - identify pattern of categorizing (is there a category associated with certain words)\n",
    "    - check if there any uncatgorized comments (find a way to categorize them or remove them)\n",
    "-\n",
    "#### Further Cleaning:\n",
    "    - based on the analysis conclusion, i will modify the data by removeing or modifying specific values (removeing ceratin words or categorizing uncategorized comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5fe061",
   "metadata": {},
   "source": [
    "### 1. Analysis & Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29a6ed3",
   "metadata": {},
   "source": [
    "- usnig Regular Expression, the above function removes  links, punctiuation and special characters, emails, and numbers\n",
    "- now the data will be analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16bf1c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         explanationwhy the edits made under my usernam...\n",
       "1         daww he matches this background colour im seem...\n",
       "2         hey man im really not trying to edit war its j...\n",
       "3         morei cant make any real suggestions on improv...\n",
       "4         you sir are my hero any chance you remember wh...\n",
       "                                ...                        \n",
       "159566    and for the second time of asking when your vi...\n",
       "159567    you should be ashamed of yourself that is a ho...\n",
       "159568    spitzer umm theres no actual article for prost...\n",
       "159569    and it looks like it was actually you who put ...\n",
       "159570    and  i really dont think you understand  i cam...\n",
       "Name: comment_text, Length: 159571, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean(text):\n",
    "    clean_text = text.lower()\n",
    "    #remove links\n",
    "    clean_text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", clean_text)\n",
    "    #remove punctiuation and special characters\n",
    "    clean_text = re.sub(r\"[^a-zA-Z0-9 ]\", \"\", clean_text)\n",
    "    #remove emails\n",
    "    clean_text = re.sub(r\"([\\w\\.\\-\\_]+@[\\w\\.\\-\\_]+)\", \"\", clean_text) \n",
    "    #remove numbers\n",
    "    clean_text = re.sub(r\"(\\d+)\", \"\", clean_text)    \n",
    "    return clean_text\n",
    "df[\"comment_text\"] = df[\"comment_text\"].apply(clean)\n",
    "df[\"comment_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b3ba81",
   "metadata": {},
   "source": [
    "- as I saw the phrase \"daww\" in the second row and since it is an irregular phrase, I looked for more in other rows\n",
    "- but there were non\n",
    "- I did the same with the phrase \"umm\", and it was present in 3533 rows\n",
    "- I decided to add these \"daww\" and \"umm\" to the stop words that will be removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03817ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>daww he matches this background colour im seem...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "1  000103f0d9cfb60f  daww he matches this background colour im seem...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "1             0        0       0       0              0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df[\"comment_text\"].str.contains('daww')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9abab59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>001735f961a23fc4</td>\n",
       "      <td>sure but the lead must briefly summarize arme...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>00328eadb85b3010</td>\n",
       "      <td>minimization of textile effluenta proposed del...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>003d77a20601cec1</td>\n",
       "      <td>thanks much  however if its been resolved why ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>00587c559177dcf2</td>\n",
       "      <td>replyare you being facetious if not you would ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>00a1aabcab9d44a0</td>\n",
       "      <td>yes they are indeed  ive replaced that second ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159183</th>\n",
       "      <td>f9c943b15a015b99</td>\n",
       "      <td>important notice the actions that you have tak...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159203</th>\n",
       "      <td>fa35aba966be5657</td>\n",
       "      <td>fourth examination th december  additional sta...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159455</th>\n",
       "      <td>fdefec22131d625a</td>\n",
       "      <td>i would value your opinion on this the recent ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159496</th>\n",
       "      <td>fefb3cc85d4d74b2</td>\n",
       "      <td>from wpthird opinion the inclusion of that se...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>spitzer umm theres no actual article for prost...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3533 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "40      001735f961a23fc4   sure but the lead must briefly summarize arme...   \n",
       "80      00328eadb85b3010  minimization of textile effluenta proposed del...   \n",
       "98      003d77a20601cec1  thanks much  however if its been resolved why ...   \n",
       "142     00587c559177dcf2  replyare you being facetious if not you would ...   \n",
       "251     00a1aabcab9d44a0  yes they are indeed  ive replaced that second ...   \n",
       "...                  ...                                                ...   \n",
       "159183  f9c943b15a015b99  important notice the actions that you have tak...   \n",
       "159203  fa35aba966be5657  fourth examination th december  additional sta...   \n",
       "159455  fdefec22131d625a  i would value your opinion on this the recent ...   \n",
       "159496  fefb3cc85d4d74b2   from wpthird opinion the inclusion of that se...   \n",
       "159568  ffee36eab5c267c9  spitzer umm theres no actual article for prost...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "40          0             0        0       0       0              0  \n",
       "80          0             0        0       0       0              0  \n",
       "98          0             0        0       0       0              0  \n",
       "142         0             0        0       0       0              0  \n",
       "251         0             0        0       0       0              0  \n",
       "...       ...           ...      ...     ...     ...            ...  \n",
       "159183      0             0        0       0       0              0  \n",
       "159203      0             0        0       0       0              0  \n",
       "159455      0             0        0       0       0              0  \n",
       "159496      0             0        0       0       0              0  \n",
       "159568      0             0        0       0       0              0  \n",
       "\n",
       "[3533 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df[\"comment_text\"].str.contains('umm')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51492256",
   "metadata": {},
   "source": [
    "- as part of the analysis, I check if there are uncategorized comments\n",
    "- there are 143,346 rows\n",
    "- i will look for patterns in the categorized comments to try to lable them manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6041f13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>explanationwhy the edits made under my usernam...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>daww he matches this background colour im seem...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>hey man im really not trying to edit war its j...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>morei cant make any real suggestions on improv...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>ffe987279560d7ff</td>\n",
       "      <td>and for the second time of asking when your vi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>ffea4adeee384e90</td>\n",
       "      <td>you should be ashamed of yourself that is a ho...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>spitzer umm theres no actual article for prost...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>and it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>and  i really dont think you understand  i cam...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143346 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "0       0000997932d777bf  explanationwhy the edits made under my usernam...   \n",
       "1       000103f0d9cfb60f  daww he matches this background colour im seem...   \n",
       "2       000113f07ec002fd  hey man im really not trying to edit war its j...   \n",
       "3       0001b41b1c6bb37e  morei cant make any real suggestions on improv...   \n",
       "4       0001d958c54c6e35  you sir are my hero any chance you remember wh...   \n",
       "...                  ...                                                ...   \n",
       "159566  ffe987279560d7ff  and for the second time of asking when your vi...   \n",
       "159567  ffea4adeee384e90  you should be ashamed of yourself that is a ho...   \n",
       "159568  ffee36eab5c267c9  spitzer umm theres no actual article for prost...   \n",
       "159569  fff125370e4aaaf3  and it looks like it was actually you who put ...   \n",
       "159570  fff46fc426af1f9a  and  i really dont think you understand  i cam...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0           0             0        0       0       0              0  \n",
       "1           0             0        0       0       0              0  \n",
       "2           0             0        0       0       0              0  \n",
       "3           0             0        0       0       0              0  \n",
       "4           0             0        0       0       0              0  \n",
       "...       ...           ...      ...     ...     ...            ...  \n",
       "159566      0             0        0       0       0              0  \n",
       "159567      0             0        0       0       0              0  \n",
       "159568      0             0        0       0       0              0  \n",
       "159569      0             0        0       0       0              0  \n",
       "159570      0             0        0       0       0              0  \n",
       "\n",
       "[143346 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[(df[\"toxic\"] == 0) & (df[\"severe_toxic\"] == 0) & (df[\"obscene\"] == 0) & (df[\"threat\"] == 0) & (df[\"insult\"] == 0) & (df[\"identity_hate\"] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98191106",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>cocksucker before you piss around on my work</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>001810bf8c45bf5f</td>\n",
       "      <td>you are gay or antisemmitian archangel white t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>00190820581d90ce</td>\n",
       "      <td>fuck your filthy mother in the ass dry</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>001dc38a83d420cf</td>\n",
       "      <td>get fucked up get fuckeeed up  got a drink tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0020e7119b96eeeb</td>\n",
       "      <td>stupid peace of shit stop deleting my stuff as...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159411</th>\n",
       "      <td>fd2f53aafe8eefcc</td>\n",
       "      <td>fat piece of shit you obese piece of shit i th...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159493</th>\n",
       "      <td>fef142420a215b90</td>\n",
       "      <td>fucking faggot lolwat</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159494</th>\n",
       "      <td>fef4cf7ba0012866</td>\n",
       "      <td>our previous conversation you fucking shit ea...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159541</th>\n",
       "      <td>ffa33d3122b599d6</td>\n",
       "      <td>your absurd edits your absurd edits on great w...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159554</th>\n",
       "      <td>ffbdbb0483ed0841</td>\n",
       "      <td>and im going to keep posting the stuff u delet...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8449 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "6       0002bcb3da6cb337       cocksucker before you piss around on my work   \n",
       "42      001810bf8c45bf5f  you are gay or antisemmitian archangel white t...   \n",
       "43      00190820581d90ce             fuck your filthy mother in the ass dry   \n",
       "51      001dc38a83d420cf  get fucked up get fuckeeed up  got a drink tha...   \n",
       "55      0020e7119b96eeeb  stupid peace of shit stop deleting my stuff as...   \n",
       "...                  ...                                                ...   \n",
       "159411  fd2f53aafe8eefcc  fat piece of shit you obese piece of shit i th...   \n",
       "159493  fef142420a215b90                              fucking faggot lolwat   \n",
       "159494  fef4cf7ba0012866   our previous conversation you fucking shit ea...   \n",
       "159541  ffa33d3122b599d6  your absurd edits your absurd edits on great w...   \n",
       "159554  ffbdbb0483ed0841  and im going to keep posting the stuff u delet...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "6           1             1        1       0       1              0  \n",
       "42          1             0        1       0       1              1  \n",
       "43          1             0        1       0       1              0  \n",
       "51          1             0        1       0       0              0  \n",
       "55          1             1        1       0       1              0  \n",
       "...       ...           ...      ...     ...     ...            ...  \n",
       "159411      1             0        1       0       1              0  \n",
       "159493      1             0        1       0       1              0  \n",
       "159494      1             0        1       0       1              1  \n",
       "159541      1             0        1       0       1              0  \n",
       "159554      1             0        1       0       1              0  \n",
       "\n",
       "[8449 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df[\"obscene\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bc16c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'get fucked up get fuckeeed up  got a drink that you cant put down get fuck up get fucked up  im fucked up right now'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"comment_text\"][51]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129c8c32",
   "metadata": {},
   "source": [
    "- the cell above shows obscene categorized comments, which are 8449\n",
    "- as shown, words like \"fuck\", \"ass\", \"shit\", \"asshole\", \"hell\" are present in all the obscene comments \n",
    "- so that can be used as a condition to lable the uncategorized comments\n",
    "- from the couple of rows shown, I noticed that they are all categorized as toxic as well\n",
    "- so I will check if this is true for the rest or not by checking if comments categorized toxic and obscene are the same number (8449 rows)\n",
    "- in this way comments categorized obscene will be automatically categorized as toxic as well  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea6ec25c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>cocksucker before you piss around on my work</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>001810bf8c45bf5f</td>\n",
       "      <td>you are gay or antisemmitian archangel white t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>00190820581d90ce</td>\n",
       "      <td>fuck your filthy mother in the ass dry</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>001dc38a83d420cf</td>\n",
       "      <td>get fucked up get fuckeeed up  got a drink tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0020e7119b96eeeb</td>\n",
       "      <td>stupid peace of shit stop deleting my stuff as...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159411</th>\n",
       "      <td>fd2f53aafe8eefcc</td>\n",
       "      <td>fat piece of shit you obese piece of shit i th...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159493</th>\n",
       "      <td>fef142420a215b90</td>\n",
       "      <td>fucking faggot lolwat</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159494</th>\n",
       "      <td>fef4cf7ba0012866</td>\n",
       "      <td>our previous conversation you fucking shit ea...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159541</th>\n",
       "      <td>ffa33d3122b599d6</td>\n",
       "      <td>your absurd edits your absurd edits on great w...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159554</th>\n",
       "      <td>ffbdbb0483ed0841</td>\n",
       "      <td>and im going to keep posting the stuff u delet...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7926 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "6       0002bcb3da6cb337       cocksucker before you piss around on my work   \n",
       "42      001810bf8c45bf5f  you are gay or antisemmitian archangel white t...   \n",
       "43      00190820581d90ce             fuck your filthy mother in the ass dry   \n",
       "51      001dc38a83d420cf  get fucked up get fuckeeed up  got a drink tha...   \n",
       "55      0020e7119b96eeeb  stupid peace of shit stop deleting my stuff as...   \n",
       "...                  ...                                                ...   \n",
       "159411  fd2f53aafe8eefcc  fat piece of shit you obese piece of shit i th...   \n",
       "159493  fef142420a215b90                              fucking faggot lolwat   \n",
       "159494  fef4cf7ba0012866   our previous conversation you fucking shit ea...   \n",
       "159541  ffa33d3122b599d6  your absurd edits your absurd edits on great w...   \n",
       "159554  ffbdbb0483ed0841  and im going to keep posting the stuff u delet...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "6           1             1        1       0       1              0  \n",
       "42          1             0        1       0       1              1  \n",
       "43          1             0        1       0       1              0  \n",
       "51          1             0        1       0       0              0  \n",
       "55          1             1        1       0       1              0  \n",
       "...       ...           ...      ...     ...     ...            ...  \n",
       "159411      1             0        1       0       1              0  \n",
       "159493      1             0        1       0       1              0  \n",
       "159494      1             0        1       0       1              1  \n",
       "159541      1             0        1       0       1              0  \n",
       "159554      1             0        1       0       1              0  \n",
       "\n",
       "[7926 rows x 8 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[(df[\"toxic\"] == 1)  & (df[\"obscene\"] == 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a6147c",
   "metadata": {},
   "source": [
    "- as shown above, there are 7926 comments that are both toxic and obscene\n",
    "- so there is around 500 comments that are obscene but not toxic\n",
    "- nevertheless, this is a rough estimation that can be used \n",
    "- below, I will lable the uncategorized comments that include \"fuck\", \"ass\", \"shit\", \"asshole\", or \"hell\" as obscene and toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bf07c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_obscene(df):\n",
    "    condition = (df[\"comment_text\"].str.contains(\"fuck|ass|shit|asshole|hell\")) & (df[\"toxic\"] == 0) & (df[\"severe_toxic\"] == 0) & (df[\"obscene\"] == 0) & (df[\"threat\"] == 0) & (df[\"insult\"] == 0) & (df[\"identity_hate\"] == 0)\n",
    "    df.loc[condition, [\"toxic\", \"obscene\"]] = 1\n",
    "    return df\n",
    "df = categorize_obscene(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e97e52e",
   "metadata": {},
   "source": [
    "- the function above sets the condition which is that the comment is not labled and contains any of the words mentioned\n",
    "- then sets toxic and obscene columns to 1 for the rows that meet the condition \n",
    "- as shown below, comments labled obscene and toxic are now 26494\n",
    "- they were 7,926 before, which is an 18,568 increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdba54c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>cocksucker before you piss around on my work</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00078f8ce7eb276d</td>\n",
       "      <td>juelz santanas agein  juelz santana was  years...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>000c0dfd995809fa</td>\n",
       "      <td>snowflakes are not always symmetrical under g...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>000ffab30195c5e1</td>\n",
       "      <td>yes because the mother of the child in the cas...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>001363e1dbe91225</td>\n",
       "      <td>i was able to post the above list so quickly b...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159524</th>\n",
       "      <td>ff66383a2793fa14</td>\n",
       "      <td>sorry i find that theres nothing honorable abo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159541</th>\n",
       "      <td>ffa33d3122b599d6</td>\n",
       "      <td>your absurd edits your absurd edits on great w...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159554</th>\n",
       "      <td>ffbdbb0483ed0841</td>\n",
       "      <td>and im going to keep posting the stuff u delet...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159557</th>\n",
       "      <td>ffc7bbb177c3c966</td>\n",
       "      <td>it is my opinion that that happens to be offto...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159562</th>\n",
       "      <td>ffd72e9766c09c97</td>\n",
       "      <td>auto guides and the motoring press are not go...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26494 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "6       0002bcb3da6cb337       cocksucker before you piss around on my work   \n",
       "15      00078f8ce7eb276d  juelz santanas agein  juelz santana was  years...   \n",
       "22      000c0dfd995809fa   snowflakes are not always symmetrical under g...   \n",
       "27      000ffab30195c5e1  yes because the mother of the child in the cas...   \n",
       "33      001363e1dbe91225  i was able to post the above list so quickly b...   \n",
       "...                  ...                                                ...   \n",
       "159524  ff66383a2793fa14  sorry i find that theres nothing honorable abo...   \n",
       "159541  ffa33d3122b599d6  your absurd edits your absurd edits on great w...   \n",
       "159554  ffbdbb0483ed0841  and im going to keep posting the stuff u delet...   \n",
       "159557  ffc7bbb177c3c966  it is my opinion that that happens to be offto...   \n",
       "159562  ffd72e9766c09c97   auto guides and the motoring press are not go...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "6           1             1        1       0       1              0  \n",
       "15          1             0        1       0       0              0  \n",
       "22          1             0        1       0       0              0  \n",
       "27          1             0        1       0       0              0  \n",
       "33          1             0        1       0       0              0  \n",
       "...       ...           ...      ...     ...     ...            ...  \n",
       "159524      1             0        1       0       0              0  \n",
       "159541      1             0        1       0       1              0  \n",
       "159554      1             0        1       0       1              0  \n",
       "159557      1             0        1       0       0              0  \n",
       "159562      1             0        1       0       0              0  \n",
       "\n",
       "[26494 rows x 8 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[(df[\"toxic\"] == 1)  & (df[\"obscene\"] == 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2ee990",
   "metadata": {},
   "source": [
    "- as shown below, the total unlabled columns were 143,346 and now they are 124,778\n",
    "- the rest of the unlabled  data are unlabled indicating they are not offensive to be categorized under any of the categories\n",
    "- from the comments shown below, they don't show aggressivness and don't include offensive words like the ones mentioned earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee8347df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>explanationwhy the edits made under my usernam...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>daww he matches this background colour im seem...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>hey man im really not trying to edit war its j...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>morei cant make any real suggestions on improv...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>ffe987279560d7ff</td>\n",
       "      <td>and for the second time of asking when your vi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>ffea4adeee384e90</td>\n",
       "      <td>you should be ashamed of yourself that is a ho...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>spitzer umm theres no actual article for prost...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>and it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>and  i really dont think you understand  i cam...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124778 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "0       0000997932d777bf  explanationwhy the edits made under my usernam...   \n",
       "1       000103f0d9cfb60f  daww he matches this background colour im seem...   \n",
       "2       000113f07ec002fd  hey man im really not trying to edit war its j...   \n",
       "3       0001b41b1c6bb37e  morei cant make any real suggestions on improv...   \n",
       "4       0001d958c54c6e35  you sir are my hero any chance you remember wh...   \n",
       "...                  ...                                                ...   \n",
       "159566  ffe987279560d7ff  and for the second time of asking when your vi...   \n",
       "159567  ffea4adeee384e90  you should be ashamed of yourself that is a ho...   \n",
       "159568  ffee36eab5c267c9  spitzer umm theres no actual article for prost...   \n",
       "159569  fff125370e4aaaf3  and it looks like it was actually you who put ...   \n",
       "159570  fff46fc426af1f9a  and  i really dont think you understand  i cam...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0           0             0        0       0       0              0  \n",
       "1           0             0        0       0       0              0  \n",
       "2           0             0        0       0       0              0  \n",
       "3           0             0        0       0       0              0  \n",
       "4           0             0        0       0       0              0  \n",
       "...       ...           ...      ...     ...     ...            ...  \n",
       "159566      0             0        0       0       0              0  \n",
       "159567      0             0        0       0       0              0  \n",
       "159568      0             0        0       0       0              0  \n",
       "159569      0             0        0       0       0              0  \n",
       "159570      0             0        0       0       0              0  \n",
       "\n",
       "[124778 rows x 8 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[(df[\"toxic\"] == 0) & (df[\"severe_toxic\"] == 0) & (df[\"obscene\"] == 0) & (df[\"threat\"] == 0) & (df[\"insult\"] == 0) & (df[\"identity_hate\"] == 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3c7498",
   "metadata": {},
   "source": [
    "- continuing the analysis, i will define the most and least common words\n",
    "- the comments are first tokenized to apply the counter on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52ff9543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [explanationwhy, the, edits, made, under, my, ...\n",
       "1         [daww, he, matches, this, background, colour, ...\n",
       "2         [hey, man, im, really, not, trying, to, edit, ...\n",
       "3         [morei, cant, make, any, real, suggestions, on...\n",
       "4         [you, sir, are, my, hero, any, chance, you, re...\n",
       "                                ...                        \n",
       "159566    [and, for, the, second, time, of, asking, when...\n",
       "159567    [you, should, be, ashamed, of, yourself, that,...\n",
       "159568    [spitzer, umm, theres, no, actual, article, fo...\n",
       "159569    [and, it, looks, like, it, was, actually, you,...\n",
       "159570    [and, i, really, dont, think, you, understand,...\n",
       "Name: comment_text, Length: 159571, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"comment_text\"] = df[\"comment_text\"].apply(nltk.word_tokenize)\n",
    "df[\"comment_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42ae0254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 490086),\n",
       " ('to', 296181),\n",
       " ('of', 223797),\n",
       " ('and', 221106),\n",
       " ('a', 213713),\n",
       " ('you', 201238),\n",
       " ('i', 192393),\n",
       " ('is', 175356),\n",
       " ('that', 153583),\n",
       " ('in', 142792),\n",
       " ('it', 127627),\n",
       " ('for', 101599),\n",
       " ('not', 96168),\n",
       " ('this', 94764),\n",
       " ('on', 88881)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = Counter()\n",
    "for tokens in df[\"comment_text\"]:\n",
    "    word_counts.update(tokens)\n",
    "\n",
    "word_counts.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b10820",
   "metadata": {},
   "source": [
    "- shown above the 15 most common words, which are obviously stop words that will be removed\n",
    "- shown below the remove_stop_words function uses the list of stopwords from NLTK and I added to it the words \"daww\" and \"umm\"\n",
    "- the function takes in the tokeized comments and returns them without the stop words defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97532047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [explanationwhy, edits, made, username, hardco...\n",
       "1         [matches, background, colour, im, seemingly, s...\n",
       "2         [hey, man, im, really, trying, edit, war, guy,...\n",
       "3         [morei, cant, make, real, suggestions, improve...\n",
       "4                [sir, hero, chance, remember, page, thats]\n",
       "                                ...                        \n",
       "159566    [second, time, asking, view, completely, contr...\n",
       "159567          [ashamed, horrible, thing, put, talk, page]\n",
       "159568    [spitzer, theres, actual, article, prostitutio...\n",
       "159569    [looks, like, actually, put, speedy, first, ve...\n",
       "159570    [really, dont, think, understand, came, idea, ...\n",
       "Name: comment_text, Length: 159571, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stop_words(text):\n",
    "    custom_stopwords = [\"daww\", \"umm\"]\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stop_words.update(custom_stopwords)\n",
    "    filtered_text = [word for word in text if word not in stop_words]\n",
    "    return filtered_text\n",
    "df[\"comment_text\"] = df[\"comment_text\"].apply(remove_stop_words)   \n",
    "df[\"comment_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1d69c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('article', 53866),\n",
       " ('page', 43972),\n",
       " ('wikipedia', 33869),\n",
       " ('talk', 31130),\n",
       " ('would', 29092),\n",
       " ('please', 27891),\n",
       " ('one', 27714),\n",
       " ('like', 27572),\n",
       " ('dont', 25879),\n",
       " ('see', 21101),\n",
       " ('also', 19945),\n",
       " ('think', 19925),\n",
       " ('know', 18911),\n",
       " ('im', 18643),\n",
       " ('people', 17504)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = Counter()\n",
    "\n",
    "for tokens in df[\"comment_text\"]:\n",
    "    word_counts.update(tokens)\n",
    "\n",
    "word_counts.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78a17e7",
   "metadata": {},
   "source": [
    "- after removing the stop words, the most common words make more sense\n",
    "- and the elements of the word counter shows the frequency of each word\n",
    "- it also shows alot of repeated letters such as cccccc, kkkkkk, zzzzzzzz and the repeated pattern hahahaha\n",
    "- most of the words with count 3 or less are repeated letters like above, gibbrish, and misspelled words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20a0d1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [edits, made, username, hardcore, metallica, f...\n",
       "1         [matches, background, colour, im, seemingly, s...\n",
       "2         [hey, man, im, really, trying, edit, war, guy,...\n",
       "3         [morei, cant, make, real, suggestions, improve...\n",
       "4                [sir, hero, chance, remember, page, thats]\n",
       "                                ...                        \n",
       "159566    [second, time, asking, view, completely, contr...\n",
       "159567          [ashamed, horrible, thing, put, talk, page]\n",
       "159568    [spitzer, theres, actual, article, prostitutio...\n",
       "159569    [looks, like, actually, put, speedy, first, ve...\n",
       "159570    [really, dont, think, understand, came, idea, ...\n",
       "Name: comment_text, Length: 159571, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_infrequent_words(column, n=4):\n",
    "    word_counts = Counter()\n",
    "\n",
    "    for row in column:\n",
    "        word_counts.update(row)\n",
    "\n",
    "    frequent_words = {word for word, frequency in word_counts.items() if frequency > n}\n",
    "    \n",
    "    def filter_words(tokens):\n",
    "        return [word for word in tokens if word in frequent_words]\n",
    "\n",
    "    column = column.apply(filter_words)\n",
    "\n",
    "    return column\n",
    "df[\"comment_text\"] = remove_infrequent_words(df[\"comment_text\"])\n",
    "df[\"comment_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eadbd4",
   "metadata": {},
   "source": [
    "- after removing words that are repeated 3 time or less, the comments will be converted to numerical values based on the vocab's indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ee5fce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_indices(tokenized_text, vocab):\n",
    "    return [[vocab[token] for token in tokens] for tokens in tokenized_text]\n",
    "\n",
    "df[\"numerical_text\"] = text_to_indices(df[\"comment_text\"], word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34310422",
   "metadata": {},
   "source": [
    "### What's concluded from above is as follow:\n",
    "- the comments are clean and ready to be embedded to be the model's input\n",
    "- the categories will be set as target for the model\n",
    "\n",
    "### Next step:\n",
    "\n",
    "- \n",
    "#### Preparing Input:\n",
    "    - set the input\n",
    "- \n",
    "#### Preparing Target:\n",
    "    - set categories columns as the target\n",
    "- \n",
    "#### Split Data:\n",
    "    - split data for training and testing\n",
    "    - pad the input to be trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3e7bd5",
   "metadata": {},
   "source": [
    "### 2. Preparing Input & Target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e219e866",
   "metadata": {},
   "source": [
    "- the input of the transformer will be the numerical_text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e324a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [9745, 9520, 1760, 147, 35, 979, 3851, 452, 38...\n",
       "1         [341, 696, 207, 18643, 164, 336, 12005, 31130,...\n",
       "2         [2505, 2643, 18643, 9157, 4975, 17377, 3751, 1...\n",
       "3         [14, 6101, 12879, 3679, 715, 448, 108, 9788, 3...\n",
       "4                       [537, 217, 1068, 2067, 43972, 6823]\n",
       "                                ...                        \n",
       "159566    [2821, 15141, 1305, 3788, 2194, 113, 619, 4573...\n",
       "159567                 [161, 251, 5209, 6139, 31130, 43972]\n",
       "159568             [5, 3249, 1648, 53866, 66, 217, 27, 153]\n",
       "159569    [2139, 27572, 5958, 6139, 4817, 10566, 3274, 8...\n",
       "159570    [9157, 25879, 19925, 4834, 2097, 3130, 3369, 8...\n",
       "Name: numerical_text, Length: 159571, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Input = df[\"numerical_text\"]\n",
    "Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77617e1",
   "metadata": {},
   "source": [
    "- because each category is in a seperate column, they will be combined to represent a comment's category (target)\n",
    "- in the following cell, the columns merged into one column \"Target\" and its values are stored as lists and converted to a tansor\n",
    "- in this way each comment has a corresponding list with 6 zeros or/and ones indicating its category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5e6a0f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([159571, 6])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_columns():\n",
    "    columns_to_merge = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "    df[\"Target\"] = df[columns_to_merge].values.tolist()\n",
    "    return torch.tensor(df[\"Target\"], dtype=torch.int32)\n",
    "target = merge_columns()\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656d11d8",
   "metadata": {},
   "source": [
    "- the input and target are split to 40% training and 60% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93564259",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Input,target, test_size = 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e5c017",
   "metadata": {},
   "source": [
    "- the input used for training which is the X_train is padded using the function below\n",
    "- the function converts the column to a tensor, then pads it with zero using pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42dc5f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_column(column):\n",
    "    # Convert the column to a tensor\n",
    "    column_data = [torch.tensor(sequence, dtype=torch.int32) for sequence in column]\n",
    "\n",
    "    # Pad the tensors using pad_sequence\n",
    "    padded_data = pad_sequence(column_data, padding_value = 0)\n",
    "\n",
    "    return padded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97c79a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2139,    93, 27891,  ...,   302, 15489,  5812],\n",
       "        [   32,    73,  8199,  ...,    12,  2941, 10405],\n",
       "        [ 4674,  1042,  5608,  ...,  2194,    82, 33869],\n",
       "        ...,\n",
       "        [    0,     0,     0,  ...,     0,     0,     0],\n",
       "        [    0,     0,     0,  ...,     0,     0,     0],\n",
       "        [    0,     0,     0,  ...,     0,     0,     0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_padded = pad_column(X_train)\n",
    "X_train_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "736c8cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1250, 63828])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc92da48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2139,    32,  4674,  ...,     0,     0,     0],\n",
       "        [   93,    73,  1042,  ...,     0,     0,     0],\n",
       "        [27891,  8199,  5608,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  302,    12,  2194,  ...,     0,     0,     0],\n",
       "        [15489,  2941,    82,  ...,     0,     0,     0],\n",
       "        [ 5812, 10405, 33869,  ...,     0,     0,     0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_padded = np.transpose(X_train_padded)\n",
    "X_train_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d9ac3ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([63828, 1250])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e09534c",
   "metadata": {},
   "source": [
    "- cast y_train to torch's type integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b19d92f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = y_train.type(torch.LongTensor)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04afc158",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([63828, 6])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69170a1d",
   "metadata": {},
   "source": [
    "- will do the same for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e995417f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    7,  8962,   329,  ...,     0,     0,     0],\n",
       "        [   17,  6139,   729,  ...,     0,     0,     0],\n",
       "        [   21,   846,  2107,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [ 2505,  5837,  2194,  ...,     0,     0,     0],\n",
       "        [12005,  5907,   912,  ...,     0,     0,     0],\n",
       "        [  230,   500,   450,  ...,     0,     0,     0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_padded = np.transpose(pad_column(X_test))\n",
    "X_test_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2317add8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = y_test.type(torch.LongTensor)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33b0e8d",
   "metadata": {},
   "source": [
    "### What's concluded from above is as follow:\n",
    "- the input and target of the model are set\n",
    "- the data is split into training and testing\n",
    "\n",
    "### Next step:\n",
    "\n",
    "- \n",
    "#### Building the model:\n",
    "    - build all the transformer's components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c72d0f",
   "metadata": {},
   "source": [
    "### 3. Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06728193",
   "metadata": {},
   "source": [
    "- \n",
    "#### The steps of building the transformer model is as follow:\n",
    "    - Building the basic blocks: Multi-head Attention, Position-wise Feed-Forward Networks, Positional Encoding\n",
    "    - Building the Encoder block\n",
    "    - Building the Decoder block\n",
    "    - Combining the Encoder and Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96acfa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    # here a class is initialized by taking the embedding dimension and number of heads\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # make sure the model's dimention is divisible by the number of heads\n",
    "        # to be abel to compute the embedding size of each head as shown in line 12\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model # Dimention: 128 \n",
    "        self.num_heads = num_heads # Dimention: 8\n",
    "        # compute the dimensions of one head\n",
    "        self.d_k = d_model // num_heads #128/8 = 16 for each key,query,value\n",
    "        \n",
    "        # create a neural network layer for each vector and the number of nodes is the embedding dimension\n",
    "        # Dimention: 64 x 64 \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # the linear layer to generate the final output that would be then put into an activation function\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask = None):\n",
    "        \n",
    "        # get the dimention of the key vector and cast it to float,\n",
    "        # so the attention score would be divided by it later it\n",
    "        # here is the operation for computing the attention score\n",
    "        \n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # the if condition sets a mask to negative infinity if the mask equals zero\n",
    "        # this is done for the decoder part, where inputs ahead of the the decoder's time step would be blocked \n",
    "        # this is the Masked Multi-Head Self-Attention\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "            \n",
    "        # the attention scores are put into a softmax to compute the attention weights\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # the weighted sum of values is the multiplication of the attention weights and the value vector\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    # this method takes in a matrix with all the q (or k or v) weights for all heads\n",
    "    #and splits the matrix to separate each head's weights in a separate matrix\n",
    "    def split_heads(self, x):\n",
    "        \n",
    "        # here we get the size of the batch to use it to reshape the vectors accordingly\n",
    "        #the split involves two steps: \n",
    "        #1-reshaping: the matrix had all q (or k or v) weights of all heads in one row\n",
    "        #reshaping lets each q[i] of all heads in a separate matrix and each q[i] of a head in a separate column\n",
    "        #however, all q[i] weights of each head are not togather, and that what transpose does\n",
    "        #2-transpose: switches the position of the second and third axis \n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        \n",
    "        # here it reverses what the previous function did\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \n",
    "        # generate the Q, K, V after spliting them\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        # the output is generated by caling the scaled_dot_product_attention method\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "15a2ec9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()        \n",
    "        # here is the feed forward layer that is used twice in the transformer\n",
    "        # once in the encoder and another in the decoder\n",
    "        # in both they use the ReLU activation function\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "14e9fd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    # this function adds the position embedding to the input tensor\n",
    "    # to enable the transformer to identify words' positions using sin and cosin functions\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()        \n",
    "        # here a tensor of zeros is generated with dimension similar to the input tensor\n",
    "        # with dimensions of sequence length * embedding dimension \n",
    "        # for one token, its dimension is 1 * 128. So the pe will be the same size to be added to the input tensor\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        \n",
    "        # a tensor is generated with values from 0 to maximum sequence length to create position indices for each position\n",
    "        # then the sin values are computed from even div_term,which is a factor that scales the position indicies\n",
    "        # and the cosin function is used for odd div_term values\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    # here the stored positional encoding values are added to the input tensor    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "38b12681",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    # the encoder layer encapsulates its compnents: multi-head attention layer, the feed forward layer \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)        \n",
    "        # and adds two normalization layers: one after the multi-head attention and one after the feed forward\n",
    "        # normalization is a regularization technique to smooth values\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)       \n",
    "        # in addition to the drop out layer with each normalization layer\n",
    "        # the dropout is used to prevent overfitting by droping out some of the network's nodes\n",
    "        # it could be a hyperparameter to optemize the model later\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        # the encoder layer runs as follow:\n",
    "        # 1A- input (word embedding + posional encoding) is fed into the multi-head attention layer\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        # 1B- input is also fed into the normalization and dropout layers, which is the residual connection\n",
    "        # 2- Multi-head attention layer outputs the attention weights, which are fed into the normalization and dropout layers \n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        # 3- the output of the previous step goes into the feed forward layer\n",
    "        ff_output = self.feed_forward(x)\n",
    "        # 4- finally, the output goes into the second normalization and dropout layers\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7029834",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        # the decoder layer consists of two multi-head attention layers: one is a masked MHAL and the another regular MHAL\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        # a feed forward layer\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        # and three normalization layers and corresponding dropout layers\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        # the decoder layer runs as follow:\n",
    "        # 1A- the target goes into the first masked MHAL \n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        # 1B- the target goes into the normalization and dropout layers, which is the first residual connection \n",
    "        # 2- the output from the first MHAL goes into the normalization and dropout layers\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        # 3- the encoder block's output goes into the second MHAL       \n",
    "        # 4- output of the first residual connection and  utput of the first MHAL go into the second residual connection\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        # 5- output from the second residual connection goes into the feed forward\n",
    "        # 6- and finally, the output goes into the third residual connection\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d845e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    # the transformer takes in all parameters including: source vocab size, target vocab size, embedding dimension,\n",
    "    # number of heads, number of layers of encoder and decoder, feed-forward layer dimension, maximum sequence length,\n",
    "    # and dropout rate\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        # the transformer takes in the components: embedding layers for the encoder and decoder and the positional encoding\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "        # here is the list of encoder and decoder layers # which are 6 for each \n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        # linear layer to map the output of the decoder to the target vocabulary size\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        # dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    #this method generates masks \n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a458b979",
   "metadata": {},
   "source": [
    "### What's concluded from above is as follow:\n",
    "- all parts of the model's components are built \n",
    "\n",
    "### Next step:\n",
    "\n",
    "- \n",
    "#### Train the model:\n",
    "    - prepare the data by batching\n",
    "    - identify the model's arguments, such as source and target vocab size, embedding dimension, number of heads and more\n",
    "    - train loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e545533",
   "metadata": {},
   "source": [
    "### 4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252e6bef",
   "metadata": {},
   "source": [
    "- get the size of the vocabulary of the comments using the function below\n",
    "- it iterates over the elements from the Counter function and adds 1 at each iteration\n",
    "- returning the total number of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "696415cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_size(text):\n",
    "    element_iterator = text.elements()\n",
    "    element_count = sum(1 for i in element_iterator)\n",
    "    return element_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda3801a",
   "metadata": {},
   "source": [
    "- the vocabulary size is 5,413,393\n",
    "- it will be used later as an argument for the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "355f706e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5413393"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vocab_size = get_vocab_size(word_counts)\n",
    "input_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd06712",
   "metadata": {},
   "source": [
    "- PyTorch's Dataset class is used to as the first step to batch the model's data\n",
    "- then PyTorch loader uses its methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d68c445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "    \n",
    "    # this method returns the total number of samples \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    # this method retrieves individual samples (the comment and its category) from the dataset during training\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.targets[index]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "22e9c963",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = iter(DataLoader(CustomDataset(X_train_padded, y_train), batch_size=15, shuffle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "690e571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model's arguments\n",
    "src_vocab_size = input_vocab_size\n",
    "tgt_vocab_size = 6\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = len(X_train_padded[0])\n",
    "dropout = 0.1\n",
    "\n",
    "# initialize the model\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "71834a14",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder_embedding): Embedding(5413393, 128)\n",
       "  (decoder_embedding): Embedding(6, 128)\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0-5): 6 x EncoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (feed_forward): PositionWiseFeedForward(\n",
       "        (fc1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder_layers): ModuleList(\n",
       "    (0-5): 6 x DecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (feed_forward): PositionWiseFeedForward(\n",
       "        (fc1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=128, out_features=6, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fe4c8b",
   "metadata": {},
   "source": [
    "- the traininng loop does's generate any errors, however it keeps running without diplyaing anything \n",
    "- I restarted the whole code countless times, but nothing changes\n",
    "- maybe it needed more time to run\n",
    "- I manually interrupted the cell as shown to close the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "62233ba8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m src_data, tgt_data \u001b[38;5;241m=\u001b[39m batch_data  \n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# compute loss\u001b[39;00m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, tgt_vocab_size), tgt_data[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [39]\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, tgt)\u001b[0m\n\u001b[0;32m     33\u001b[0m enc_output \u001b[38;5;241m=\u001b[39m src_embedded\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m enc_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_layers:\n\u001b[1;32m---> 35\u001b[0m     enc_output \u001b[38;5;241m=\u001b[39m \u001b[43menc_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m dec_output \u001b[38;5;241m=\u001b[39m tgt_embedded\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dec_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_layers:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [37]\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# the encoder layer runs as follow:\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# 1A- input (word embedding + posional encoding) is fed into the multi-head attention layer\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# 1B- input is also fed into the normalization and dropout layers, which is the residual connection\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# 2- Multi-head attention layer outputs the attention weights, which are fed into the normalization and dropout layers \u001b[39;00m\n\u001b[0;32m     22\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attn_output))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, Q, K, V, mask)\u001b[0m\n\u001b[0;32m     69\u001b[0m V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_heads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_v(V))\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# the output is generated by caling the scaled_dot_product_attention method\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_o(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine_heads(attn_output))\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36mMultiHeadAttention.scaled_dot_product_attention\u001b[1;34m(self, Q, K, V, mask)\u001b[0m\n\u001b[0;32m     35\u001b[0m     attn_scores \u001b[38;5;241m=\u001b[39m attn_scores\u001b[38;5;241m.\u001b[39mmasked_fill(mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1e9\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# the attention scores are put into a softmax to compute the attention weights\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m attn_probs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# the weighted sum of values is the multiplication of the attention weights and the value vector\u001b[39;00m\n\u001b[0;32m     41\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(attn_probs, V)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(3):\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for i, batch_data in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        # unpack the batch\n",
    "        src_data, tgt_data = batch_data  \n",
    "        \n",
    "        # forward pass\n",
    "        output = transformer(src_data, tgt_data[:, :-1])\n",
    "        \n",
    "        # compute loss\n",
    "        loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "        \n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # print the average loss for each epoch\n",
    "    print(f\"Epoch: {epoch + 1}, Loss: {total_loss / len(loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50328b9f",
   "metadata": {},
   "source": [
    "### 5. Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75c6767",
   "metadata": {},
   "source": [
    "- create a test loader with the test input and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446832c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = iter(DataLoader(CustomDataset(X_test_padded, y_test), batch_size=15, shuffle=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da0ae8b",
   "metadata": {},
   "source": [
    "- this is the testing loop, that will evaluate the model using the 60%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ff7d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "transformer.eval()\n",
    "total_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch_data in enumerate(test_loader):\n",
    "        src_data, tgt_data = batch_data\n",
    "\n",
    "        # forward pass\n",
    "        output = transformer(src_data, tgt_data[:, :-1])\n",
    "\n",
    "        # compute loss\n",
    "        loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "test_loss = total_loss / len(data_loader)\n",
    "test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a244887a",
   "metadata": {},
   "source": [
    "### 6. Optimizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaeb0f4",
   "metadata": {},
   "source": [
    "- optimizing the model will envolve changing the parameters of the model such as the model's dimension and batch size\n",
    "- regularization is also another way, where the dropout rate can be changed to prevent overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
